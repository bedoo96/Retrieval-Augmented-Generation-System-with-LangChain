{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting Pillow\n",
      "  Using cached pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "Collecting huggingface-hub>=0.20.0\n",
      "  Using cached huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Collecting torch>=1.11.0\n",
      "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:10\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.1.0\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0m\n",
      "\u001b[?25hCollecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Collecting tokenizers<0.21,>=0.20\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Installing collected packages: mpmath, tqdm, threadpoolctl, sympy, scipy, safetensors, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, joblib, fsspec, filelock, triton, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence-transformers\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.0.0 filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.3 jinja2-3.1.4 joblib-1.4.2 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.3.1 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.20.3 torch-2.5.1 tqdm-4.67.1 transformers-4.46.3 triton-3.1.0\n"
     ]
    }
   ],
   "source": [
    "# ! pip install langchain\n",
    "# ! pip install -qU langchain_community beautifulsoup4\n",
    "! pip install tiktoken\n",
    "! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM instance created successfully.\n",
      "LLM is ready for use!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "class Models:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Models class.\"\"\"\n",
    "        self.llm = None  # Initialize llm attribute as None\n",
    "\n",
    "    def mistral(self):\n",
    "        \"\"\"Create and return an instance of the Mistral LLM.\"\"\"\n",
    "        try:\n",
    "            self.llm = OllamaLLM(base_url=\"http://localhost:11434\", model=\"llama3.1\", temperature=0)\n",
    "            print(\"LLM instance created successfully.\")\n",
    "            return self.llm\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while creating the LLM instance: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Create an instance of the Models class\n",
    "llm_check = Models()  # Create an instance of the Models class\n",
    "\n",
    "# Initialize the Mistral LLM using the instance\n",
    "llm = llm_check.mistral()\n",
    "\n",
    "if llm:\n",
    "    print(\"LLM is ready for use!\")\n",
    "else:\n",
    "    print(\"Failed to initialize the LLM.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://dmap.ncdr.nat.gov.tw/1109/disaster-topics/%E7%81%BD%E5%AE%B3%E8%AD%A6%E6%88%92%E5%80%BC/', 'title': '3D災害潛勢地圖', 'description': '3D災害潛勢地圖，「災害潛勢」指某一地區過去曾發生災害，或未來有較高的致災機會。', 'language': 'zh-hant'}, page_content='\\n\\n\\n\\n\\n\\n3D災害潛勢地圖\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX;visibility:hidden\">\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3D災害潛勢地圖\\n\\n\\n\\n\\n\\n\\n\\n情境說明\\n\\n情境應用\\n背景說明\\n最新消息\\n\\n\\n\\n災害主題\\n\\n哪裡容易淹水（淹水潛勢）\\n山崩、土石流（坡地災害潛勢）\\n斷層、土壤液化\\n海岸災害 海嘯溢淹\\n火山災害\\n災害警戒值\\n\\n\\n\\n地圖查詢\\n\\n\\n資料分析與下載\\n\\n縣市層級\\n鄉鎮層級\\n聚落層級\\n警戒值查詢\\n下載清單\\n\\n\\n\\n相關連結\\n\\n本中心其他網站\\n部會署防災相關\\n國外防救災網站\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n情境說明\\n\\n情境應用\\n背景說明\\n最新消息\\n\\n\\n\\n災害主題\\n\\n哪裡容易淹水（淹水潛勢）\\n山崩、土石流（坡地災害潛勢）\\n斷層、土壤液化\\n海岸災害 海嘯溢淹\\n火山災害\\n災害警戒值\\n\\n\\n\\n地圖查詢\\n\\n\\n資料分析與下載\\n\\n縣市層級\\n鄉鎮層級\\n聚落層級\\n警戒值查詢\\n下載清單\\n\\n\\n\\n相關連結\\n\\n本中心其他網站\\n部會署防災相關\\n國外防救災網站\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                        facebook\\r\\n                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                        問卷\\r\\n                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                        mail\\r\\n                                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n首頁\\n災害主題\\n災害警戒值\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n災害警戒值\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n友善列印\\n\\n\\n\\n\\n什麼是警戒值？\\n為了讓民眾能在颱風、豪雨期間事先做好減災、避災的工作，因此分析各式的災害潛勢地圖，但是災害潛勢不代表一定會發生災害，仍須視其降雨條件而定。為此，各種災害主管機關及相關單位，針對各種災害研訂發佈警戒參考標準，以利防災人員在應變操作及民眾使用。目前臺灣使用的警戒值包含淹水警戒值、河川水位警戒值、土石流警戒基準值、重點監控路段及橋梁的警戒值，主要都是透過歷史災害事件發生時其雨量值，經過統計分析後，所得的結果，不同的降雨警戒基準值代表著該地區環境的特性，以土石流警戒基準值為例（表1），警戒值250mm之地區環境狀況相較警戒值600mm之地區環境狀況不好，所以警戒值的高低也意涵著該區災害風險的不同。\\n表1 不同警戒值地表環境特徵（資料來源：截自農村水保署）\\n\\n以下就針對目前所訂定之各種災害警戒值定義說明：\\n淹水警戒值分級定義：\\n淹水警戒即採用經驗方法，根據歷史淹水與雨量資料，訂出各鄉鎮的1、3、6、12及24小時降雨警戒值，只要觀測降雨達到該鄉鎮其中一項警戒值就會發布一級警戒，而當雨量達到略低於警戒值10～60毫米則發布二級警戒。\\n二級警戒：發布淹水警戒之鄉(鎮、市、區)如持續降雨，其轄內易淹水村里及道路可能在三小時內開始積淹水。\\n一級警戒：發布淹水警戒之鄉(鎮、市、區)如持續降雨，其轄內易淹水村里及道路可能已經開始積淹水。\\n\\xa0\\n註：淹水警戒值準確性受降雨時空分布不均、雨量站密度、地形地物、河川排水及其當時水位高低、沿海潮位、排水流路阻塞等因素影響，可配合即時雨量觀測（如QPESUMS）及當地降雨實況研判因應。\\n河川警戒水位分級定義：\\n河川警戒水位分為三級，依據不同警戒水位標準有不同的操作行為，如圖1，\\n三級警戒水位：河川水位預計未來2小時到達高灘地之水位。\\n二級警戒水位：河川水位預計未來5小時到達計畫洪水位(或堤頂)時之水位。\\n一級警戒水位：河川水位預計未來2小時到達計畫洪水位(或堤頂)時之水位。\\n註：各河川依不同防救災需求到達計畫洪水位(或堤頂)的時間有所不同。\\n\\n圖1\\xa0河川水位警戒代表的意義（資料來源：水利署）\\n土石流警戒基準值分級：\\n由於雨量是土石流發生的重要因素，故目前土石流警戒基準值係以雨量作為發布之主要參考依據。土石流警戒基準值之訂定，係依據各地區之地質特性及水文條件，並考量前期降雨與雨場分割等因素，以〝有效累積雨量〞及〝降雨強度〞兩個項目，並配合地形、地質及坡度等地文因子及機率觀念，利用統計方法計算出相類似性質集水區的雨量警戒值，然後再簡化為〝累積雨量〞，以土石流發生機率70%時之累積雨量值訂為土石流警戒值，容許誤差為正負50mm(若受地震、颱風因素影響，調降為發生機率50%，並持續加強後續觀察者)，作為土石流紅黃警戒發布及疏散避難使用（圖2）。\\n\\n圖2\\xa0土石流警戒值與黃紅警戒代表意義（資料來源：農村水保署）\\n其調整的依據包括土石流警戒基準值常態性更新、土石流警戒基準值立即性調整及短沿時強降雨警戒值動態調降等進行調整（圖3）：\\n土石流警戒基準值常態性更新\\n（一）常態性更新週期為每年一次。\\n（二）土石流警戒基準值常態性更新之考量為新增雨量事件、新增土石流事件、震度達\\xa05\\xa0級以上之地震事件及近年土石流警戒基準值調整警戒區之環境概況。\\n土石流警戒基準值立即性調整\\n（一）重大土砂災害事件發生，以重大土石流災害及重大地震事件屬之，應依據各災區情勢對土石流警戒基準值提出調整建議。\\n（二）新增土石流潛勢溪流地區，應檢討現行土石流警戒基準值有無更新之需要，若有應即檢討更新。\\n（三）新增之土石流警戒區，應根據歷年降雨資料、土石流發生事件及其地文資料等，以訂定土石流警戒基準值。\\n短沿時強降雨警戒值動態調降\\n近年來因應短沿時強降雨的影響，98年起實施警戒值動態調降，分成一級調整、二級調整、三級調整，說明如下：\\n\\n一級調整（近3小時累積雨量>200mm）：原警戒基準值400mm以下（含）調降100mm，原警戒基準值400mm以上（不含）調降150mm。\\n二級調整（近3小時累積雨量>150mm）：原警戒基準值400mm以下（含）調降50mm，原警戒基準值400mm以上（不含）調降100mm。\\n三級調整（近2小時累積雨量>100mm）：原警戒基準值400mm以下（含）調降50mm，原警戒基準值400mm以上（不含）維持不變。\\n\\n\\n圖3\\xa0土石流警戒基準值更新流程圖（資料來源：農村水保署）\\n\\xa0\\n\\n重點監控路段及橋梁的「預警值」、「警戒值」及「行動值」\\n\\n交通部公路總局以「橋梁流域管理」、「山區道路風險管理」概念為基礎，律定以依10分鐘、1小時、3小時、6小時、12小時及24小時累積雨量，訂定重點監控路段及橋梁的「預警值」、「警戒值」及「行動值」。\\n\\n預警等級（黃色警示）定義：氣象局發布劇烈天氣特報時，預測降雨量達到降雨觀測指標行動值，或實測降雨量累積達降雨觀測指標預警值時，可界定為此等級。\\n\\n『交通管制方式』：路段維持通行，並通報地方政府、當地派出所及管制站人員對現場進行警戒。\\n『用路人資訊』：為因應可能因降雨規模而提升道路應變等級，用路人請收聽警廣特別注意該路段管制應變訊息。\\n\\n警戒等級（紅色警戒）定義：視各路段不同情形，當實測降雨量累積達降雨觀測指標警戒值以上，可劃分為此類等級。\\n\\n『交通管制方式』：路段維持通行，惟可能出現零星落石及小規模土石坍流，並採隨坍隨清，管制點人員勸導遊客避免進入該區域。\\n『用路人資訊』：勸導近端用路人提早撤離，遠端用路人避免行經該路段。\\n\\n行動等級（管制封閉，黑色燈號）定義：視各路段不同情形，當實測降雨量累積達降雨觀測指標行動值以上，可劃分為此類等級。\\n\\n『交通管制方式』：路段封閉。\\n『用路人資訊』：發佈道路封閉訊息，請用路人前往安全停駐空間或緊急暫停空間或行走路況良好的替代道路。\\n\\n\\n\\n\\n相關連結\\n\\n土石流防災資訊網\\n水利署防災資訊網\\n公路局-智慧化省道即時資訊服務網\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                        新北市新店區北新路三段200號9樓\\r\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                                02-8195-8600\\r\\n                            \\n\\n\\n\\n\\r\\n                                02-8912-7766\\r\\n                            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n                地圖查詢\\r\\n            \\n\\n\\n情境說明\\n\\n情境應用\\n背景說明\\n最新消息\\n\\n\\n\\n災害主題\\n\\n哪裡容易淹水（淹水潛勢）\\n山崩、土石流（坡地災害潛勢）\\n斷層、土壤液化\\n海岸災害 海嘯溢淹\\n火山災害\\n災害警戒值\\n\\n\\n\\n\\r\\n                        資料分析與下載\\r\\n                        \\n\\n\\n縣市層級\\n鄉鎮層級\\n聚落層級\\n警戒值查詢\\n下載清單\\n\\n\\n\\n 相關連結\\n\\n本中心其他網站\\n部會署防災相關\\n國外防救災網站\\n\\n\\n\\n引用本網站內容請註明「國家災害防救科技中心」\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "#Initialize the WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://dmap.ncdr.nat.gov.tw/1109/disaster-topics/%E7%81%BD%E5%AE%B3%E8%AD%A6%E6%88%92%E5%80%BC/\")\n",
    "#$Load data and store it in a variabe\n",
    "data = loader.load()\n",
    "#display data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#Initialize the text splitter with specified parameter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap= 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chunk the loaded document into smaller chunk\n",
    "chunks = text_splitter.split_documents(data)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Embedding and Vector Databases\n",
    "\n",
    "- Once document has been successfully loaded and chunks into smaller part, we can:\n",
    "1 :\n",
    "  Choose an Embedding model to transform this human text  into vector, there, will be store in:\n",
    "2 : Vector database, the vector embeddings  will be stored in a vector store. For this   purpose, we will be using ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2540/2797426391.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf = HuggingFaceEmbeddings(\n",
      "/home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Specify the model name and additional arguments\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device' : 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize HuggingFace Embeddings\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = model_kwargs,\n",
    "    encode_kwargs = encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "embeb = hf.embed_documents(texts=['h','e'])\n",
    "#print lenght of one of embedding to check its dimension\n",
    "print(len(embeb[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
