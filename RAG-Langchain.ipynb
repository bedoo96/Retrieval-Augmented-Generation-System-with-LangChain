{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regrex in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (1.3)\n",
      "Requirement already satisfied: colorama in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from regrex) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from regrex) (1.5.2)\n",
      "Requirement already satisfied: termcolor in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from regrex) (2.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from scikit-learn->regrex) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from scikit-learn->regrex) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from scikit-learn->regrex) (1.26.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/bedoo/NLP_HuggingFace/NLP/lib/python3.10/site-packages (from scikit-learn->regrex) (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install langchain\n",
    "# ! pip install -qU langchain_community beautifulsoup4\n",
    "# ! pip install tiktoken\n",
    "# ! pip install sentence-transformers\n",
    "# ! pip install nltk\n",
    "! pip install regrex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM instance created successfully.\n",
      "LLM is ready for use!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "class Models:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Models class.\"\"\"\n",
    "        self.llm = None  # Initialize llm attribute as None\n",
    "\n",
    "    def mistral(self):\n",
    "        \"\"\"Create and return an instance of the Mistral LLM.\"\"\"\n",
    "        try:\n",
    "            self.llm = OllamaLLM(base_url=\"http://localhost:11434\", model=\"llama3.1\", temperature=0)\n",
    "            print(\"LLM instance created successfully.\")\n",
    "            return self.llm\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while creating the LLM instance: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Create an instance of the Models class\n",
    "llm_check = Models()  # Create an instance of the Models class\n",
    "\n",
    "# Initialize the Mistral LLM using the instance\n",
    "llm = llm_check.mistral()\n",
    "\n",
    "if llm:\n",
    "    print(\"LLM is ready for use!\")\n",
    "else:\n",
    "    print(\"Failed to initialize the LLM.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "#Initialize the WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://pmc.ncbi.nlm.nih.gov/articles/PMC5404248/\")\n",
    "#$Load data and store it in a variabe\n",
    "data = loader.load()\n",
    "#display data\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regex as re\n",
    "# import nltk\n",
    "\n",
    "# stopwords = set(nltk.corpus.stopwords.words('English'))\n",
    "\n",
    "# def remove__stop(tokens):\n",
    "#     return [t for t in tokens if t.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#Initialize the text splitter with specified parameter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap= 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk the loaded document into smaller chunk\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Embedding and Vector Databases\n",
    "\n",
    "- Once document has been successfully loaded and chunks into smaller part, we can:\n",
    "1 :\n",
    "  Choose an Embedding model to transform this human text  into vector, there, will be store in:\n",
    "2 : Vector database, the vector embeddings  will be stored in a vector store. For this   purpose, we will be using ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Specify the model name and additional arguments\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device' : 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize HuggingFace Embeddings\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = model_kwargs,\n",
    "    encode_kwargs = encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "embeb = hf.embed_documents(texts=['h','e'])\n",
    "#print lenght of one of embedding to check its dimension\n",
    "# print(embeb)\n",
    "print(len(embeb[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChromaDB installation\n",
    "# ! pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Initialize chroma DB to save the the vectors embedding\n",
    "vectordb = Chroma.from_documents(chunks, hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'description': 'In recent years, growth of international travel and trade, as well as climate change, has resulted in the frequent emergence and reemergence of infectious diseases such as Ebola, Zika, and MERS. In 2016, Taiwan used the Joint External Evaluation ...', 'language': 'en', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC5404248/', 'title': '\\n            Public Health Emergency Response in Taiwan - PMC\\n        '}, page_content='November20, 2016'),\n",
       " Document(metadata={'description': 'In recent years, growth of international travel and trade, as well as climate change, has resulted in the frequent emergence and reemergence of infectious diseases such as Ebola, Zika, and MERS. In 2016, Taiwan used the Joint External Evaluation ...', 'language': 'en', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC5404248/', 'title': '\\n            Public Health Emergency Response in Taiwan - PMC\\n        '}, page_content='November20, 2016'),\n",
       " Document(metadata={'description': 'In recent years, growth of international travel and trade, as well as climate change, has resulted in the frequent emergence and reemergence of infectious diseases such as Ebola, Zika, and MERS. In 2016, Taiwan used the Joint External Evaluation ...', 'language': 'en', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC5404248/', 'title': '\\n            Public Health Emergency Response in Taiwan - PMC\\n        '}, page_content='after a natural disaster occurs, infectious disease will spread suddenly. For example, after a flood, there are often many cases of Burkholderia pseudomallei, which is spread through dirty water. The')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's perform a similarity search\n",
    "vectordb.similarity_search(\"tell me about deseater\", k= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve and Generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Define the prompt template and create an LLM chain (if needed)\n",
    "prompt_template = \"Answer the question based on the context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "# Initialize the retriever with similarity score threshold\n",
    "retriever = vectordb.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8})\n",
    "\n",
    "# Initialize the ConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain(\n",
    "    llm_chain=llm_chain,\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain=load_qa_chain(llm_chain, chain_type=\"stuff\"),  \n",
    "    question_generator=llm_chain  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a query using the chain\n",
    "response = qa_chain({\"query\": \"What factors have contributed to the frequent emergence and reemergence of infectious diseases in recent years?\"})\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
